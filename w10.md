---
permalink: /W10/
---
[HOME](../)

<br>
# Top 10 List of Week 10

1. [LFSD.ova](https://lfs.vlsm.org/LFS-10.html)<br>
Checklist W10, like Checklist W09, also requires a lot of installing and decrypting tar files and also "time make"-s. some
of them take a ridiculously long amount of time that my laptop was lagging a bit. I'm still not fully sure what those files
are used for.

2. [Shared Folders](https://linuxize.com/post/how-to-install-virtualbox-guest-additions-on-debian-10/)<br>
Shared Folders are a way for files to be moved around between linux and actual local storage, to do that you need to use the
VirtualBox and also install sudo to your linux.

3. [Serial Devices](https://www.quantil.com/content-delivery-insights/content-acceleration/data-transmission/)<br>
Using Serial Devices, data is sent or received using serial data transmission, the data bits are organized in a specific order,
since they can only be sent one after another. The order of the data bits is important as it dictates how the transmission is
organized when it is received. It is viewed as a reliable data transmission method because a data bit is only sent if the previous
data bit has already been received.

4. [Parallel Devices](https://www.quantil.com/content-delivery-insights/content-acceleration/data-transmission/)<br>
Using Parallel Devices, data is sent using parallel data transmission, multiple data bits are transmitted over multiple channels
at the same time. This means that data can be sent much faster than using serial transmission methods. Given that multiple bits
are sent over multiple channels at the same time, the order in which a bit string is received can depend on various conditions,
such as proximity to the data source, user location, and bandwidth availability. Two examples of parallel interfaces can be seen
below. In the first parallel interface, the data is sent and received in the correct order. In the second parallel interface, the
data is sent in the correct order, but some bits were received faster than others.

5. [Buffering Techniques](https://www.geeksforgeeks.org/i-o-buffering-and-its-various-techniques/)<br>
A buffer is a memory area that stores data being transferred between two devices or between a device and an application. There are
a variety of buffering techniques such as Single Buffer, Double Buffer, and Circular Buffer. Each difference relating to how they
work with the Operating System and User Process.

6. [Single Buffer](https://www.geeksforgeeks.org/i-o-buffering-and-its-various-techniques/)<br>
This buffer is provided by the operating system to the system portion of the main memory. If using Block oriented devices: System
buffer takes the input; After taking the input, the block gets transferred to the user space by the process and then the process
requests for another block; Two blocks works simultaneously, when one block of data is processed by the user process, the next block
is being read in; OS can swap the processes; OS can record the data of system buffer to user processes. If using stream oriented
devices: Line- at a time operation is used for scroll made terminals. User inputs one line at a time, with a carriage return signaling
at the end of a line; Byte-at a time operation is used on forms mode, terminals when each keystroke is significant.

7. [Double Buffer](https://www.geeksforgeeks.org/i-o-buffering-and-its-various-techniques/)<br>
More than two buffers are used, the collection of buffers is itself referred to as a circular buffer; In this, the data do not directly
passed from the producer to the consumer because the data would change due to overwriting of buffers before they had been consumed; The
producer can only fill up to buffer i-1 while data in buffer i is waiting to be consumed.

8. [DMA (Direct Memory Access)](https://en.wikipedia.org/wiki/Direct_memory_access)<br>
DMA (Direct memory access) is a feature of computer systems that allows certain hardware subsystems to access main system memory
(random-accessmemory) independent of the central processing unit (CPU).Without DMA, when the CPU is using programmed input/output, it
is typically fully occupied for the entire duration of the read or write operation, and is thus unavailable to perform other work. With
DMA, the CPU first initiates the transfer, then it does other operations while the transfer is in progress, and it finally receives an
interrupt from the DMA controller (DMAC) when the operation is done. This feature is useful at any time that the CPU cannot keep up with
the rate of data transfer, or when the CPU needs to perform work while waiting for a relatively slow I/O data transfer.

9. [I/O Programming](https://www.tutorialspoint.com/operating_system/os_io_hardware.htm)<br>
When using memory-mapped I/O, the same address space is shared by memory and I/O devices. The device is connected directly to certain main
memory locations so that I/O device can transfer block of data to/from memory without going through CPU. While using memory mapped IO, OS
allocates buffer in memory and informs I/O device to use that buffer to send data to the CPU. I/O device operates asynchronously with CPU,
interrupts CPU when finished. Memory mapped IO is used for most high-speed I/O devices like disks, communication interfaces.

10. [Network Programming](https://en.wikipedia.org/wiki/Computer_network_programming)<br>
Network Programming involves writing programs that communicate with other programs across a computer network.
